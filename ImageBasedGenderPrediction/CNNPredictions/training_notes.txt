BN-3-conv-32-node-2-dens-1553635516: started training with BN and dropout added in hidden layers.

BN-3-conv-32-node-2-dens-1553636464: same settings as 1553635516 but using a larger dataset to compare difference in results (1_pct vs 5_pct datasets). Overall increased accuracy and reduced loss.

BN-3-conv-32-node-2-dens-1553641383: same setting as 1553636464 but using large dense layers to compare difference in results (2048 nodes vs 1024 nodes). accuracy dropped, loss increased. Going back to 8X dense layer size.

BN-4-conv-32-node-2-dens-1553646613: same setting as 1553636464 but using an extra set of hidden layers: slightly improved val accuracy and loss.

BN-4-conv-32-node-2-dens-1553651832: same setting as 1553646613 but using larger image size (224px vs 120px).slightly improved accuracy and loss. But training takes twice as long. Going back to 120px images for now, we can come back to 224px images when we have everything else finalized.

BN-5-conv-32-node-2-dens-1553695243: same setting as 1553646613 but added another set of hidden layers. No noticeable change in loss, small improvement in val accuracy.

BN-5-conv-32-node-3-dens-1553703610: same setting as 1553695243 but added another set of dense layers. Validation accuracy and loss are slightly worse. Going back to previous settings.

So far, the best trials are BN-5-conv-32-node-2-dens-1553695243 and BN-4-conv-32-node-2-dens-1553646613. The only difference between these two are the number of hidden layers. We want to see if the two networks will start to differentiate with larger dataset. The next two trials will be using 10pct data set on these two networks for comparison.

BN-4-conv-32-node-2-dens-1553710479: same as BN-4-conv-32-node-2-dens-1553646613 but using larger data set (10pct vs 5pct). Slightly better with larger data set.

BN-5-conv-32-node-2-dens-1553721908: same as BN-5-conv-32-node-2-dens-1553695243 but using larger data set (10pct vs 5pct). Slightly worse with larger data set.

The next two trials, we are going to re-run the previous two trials, but increasing epoch from 10 to 30. We want to compare the overfitting tendencies with higher epoch numbers.

BN-4-conv-32-node-2-dens-1553736475: same as BN-4-conv-32-node-2-dens-1553710479, but with higher epoch (30 vs 10). Overfitting started after 11 epochs.

BN-5-conv-32-node-2-dens-1553790061: same as BN-5-conv-32-node-2-dens-1553721908, but with higher epoch (30 vs 10). Good performance, still improving at 30 epochs.

Best results using weights from BN-5-conv-32-node-2-dens-1554663813.

Using 1554663813, continue training with higher drop out and lower learning rate, 155470605 show promising trend but onlymarginally better.
Using 155470605, continue training on 1554757736