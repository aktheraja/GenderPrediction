{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If using AMD GPU, switch backend to PlaidML library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='plaidml.keras.backend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using plaidml.keras.backend backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, ZeroPadding2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import time\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Experiment Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [5]      # number of conv layers\n",
    "layer_sizes = [32]     # number of nodes in a layer\n",
    "dense_layers = [2]     # number of dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open('../Dataset/df_all.pickle', 'rb')\n",
    "df_train, df_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The Keras ImageDataGenerator uses string type data label\n",
    "df_train['gender'] = df_train.gender.astype(str)\n",
    "df_test['gender'] = df_test.gender.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155169, 10) (8167, 10)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "path                object\n",
       "id                  uint16\n",
       "name                object\n",
       "dob         datetime64[ns]\n",
       "gender              object\n",
       "score1             float64\n",
       "score2             float64\n",
       "pic_date    datetime64[ns]\n",
       "region              object\n",
       "age                float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a generator to feed model with images, the X would be the path to these images. y will be the gender label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_reshape_size = 100\n",
    "input_image_root_dir = '../Dataset/imdb_crop/' # Don't forget the ending slash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "batch_size = 64\n",
    "inputShape = (image_reshape_size, image_reshape_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>score1</th>\n",
       "      <th>score2</th>\n",
       "      <th>pic_date</th>\n",
       "      <th>region</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2804</th>\n",
       "      <td>96/nm0000096_rm3980367616_1968-8-9_2008.jpg</td>\n",
       "      <td>6924</td>\n",
       "      <td>Gillian Anderson</td>\n",
       "      <td>1968-08-09</td>\n",
       "      <td>0</td>\n",
       "      <td>2.777322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>[252.35789275752745, 69.21324347932567, 293.03...</td>\n",
       "      <td>39.395744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448037</th>\n",
       "      <td>66/nm2397366_rm1173207040_1986-5-16_2011.jpg</td>\n",
       "      <td>5403</td>\n",
       "      <td>Drew Roy</td>\n",
       "      <td>1986-05-16</td>\n",
       "      <td>1</td>\n",
       "      <td>4.825077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>[231.424, 231.424, 524.288, 524.288]</td>\n",
       "      <td>24.630211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13153</th>\n",
       "      <td>36/nm0000136_rm44144128_1963-6-9_2008.jpg</td>\n",
       "      <td>9764</td>\n",
       "      <td>Johnny Depp</td>\n",
       "      <td>1963-06-09</td>\n",
       "      <td>1</td>\n",
       "      <td>3.122914</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2008-01-01</td>\n",
       "      <td>[61.61189871184332, 76.87787338980415, 198.457...</td>\n",
       "      <td>44.564912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path    id              name  \\\n",
       "2804     96/nm0000096_rm3980367616_1968-8-9_2008.jpg  6924  Gillian Anderson   \n",
       "448037  66/nm2397366_rm1173207040_1986-5-16_2011.jpg  5403          Drew Roy   \n",
       "13153      36/nm0000136_rm44144128_1963-6-9_2008.jpg  9764       Johnny Depp   \n",
       "\n",
       "              dob gender    score1  score2   pic_date  \\\n",
       "2804   1968-08-09      0  2.777322     NaN 2008-01-01   \n",
       "448037 1986-05-16      1  4.825077     NaN 2011-01-01   \n",
       "13153  1963-06-09      1  3.122914     NaN 2008-01-01   \n",
       "\n",
       "                                                   region        age  \n",
       "2804    [252.35789275752745, 69.21324347932567, 293.03...  39.395744  \n",
       "448037               [231.424, 231.424, 524.288, 524.288]  24.630211  \n",
       "13153   [61.61189871184332, 76.87787338980415, 198.457...  44.564912  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up input image generator using flow_from_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8167 images.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.1)\n",
    "\n",
    "# train_generator = datagen.flow_from_dataframe(dataframe=df_train,\n",
    "#                                             directory=input_image_root_dir,\n",
    "#                                             x_col=\"path\", y_col=\"gender\",\n",
    "#                                             subset=\"training\",\n",
    "#                                             class_mode=\"binary\",\n",
    "#                                             color_mode=\"grayscale\",\n",
    "#                                             target_size=(image_reshape_size,image_reshape_size),\n",
    "#                                             batch_size=batch_size,\n",
    "#                                             seed=1,\n",
    "#                                             shuffle=True)\n",
    "\n",
    "# val_generator = datagen.flow_from_dataframe(dataframe=df_train,\n",
    "#                                             directory=input_image_root_dir,\n",
    "#                                             x_col=\"path\", y_col=\"gender\",\n",
    "#                                             subset=\"validation\",\n",
    "#                                             class_mode=\"binary\",\n",
    "#                                             color_mode=\"grayscale\",\n",
    "#                                             target_size=(image_reshape_size,image_reshape_size),\n",
    "#                                             batch_size=batch_size,\n",
    "#                                             seed=1,\n",
    "#                                             shuffle=True)\n",
    "\n",
    "test_generator = datagen.flow_from_dataframe(dataframe=df_test, \n",
    "                                            directory=input_image_root_dir, \n",
    "                                            x_col=\"path\", y_col=None, \n",
    "                                            class_mode=None, \n",
    "                                            color_mode=\"grayscale\",\n",
    "                                            target_size=(image_reshape_size,image_reshape_size),\n",
    "                                            batch_size=1,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2-10pct-5-conv-32-node-2-dens-1554952593\n"
     ]
    }
   ],
   "source": [
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            \n",
    "            NAME = '{}-conv-{}-node-{}-dens-{}'.format(conv_layer, layer_size, dense_layer, int(time.time()))  # model name with timestamp\n",
    "            print(NAME) \n",
    "            \n",
    "            # Make sure the following subfolder exist\n",
    "            tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "            checkpoint = ModelCheckpoint('weights/{}.h5'.format(NAME), monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "            callbacks = [tensorboard, checkpoint]\n",
    "            \n",
    "            model = Sequential()\n",
    "            \n",
    "            # first layer\n",
    "            model.add(Conv2D(layer_size, (3,3), padding=\"same\", activation=\"relu\", input_shape=inputShape))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(MaxPooling2D(pool_size=(3,3)))\n",
    "            \n",
    "            # sets up additional # of conv layers\n",
    "            for _ in range(conv_layer - 1):\n",
    "                layer_size *= 2\n",
    "                model.add(Conv2D(layer_size, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Conv2D(layer_size, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "                model.add(Dropout(0.35))   # Started with 0.25, increased to 0.5\n",
    "            \n",
    "            model.add(Flatten())\n",
    "            \n",
    "            layer_size *= 4 # to get the dense layer to be 8X of last output size\n",
    "            \n",
    "            # sets up # of dense layers\n",
    "            for _ in range(dense_layer):\n",
    "                model.add(Dense(layer_size, activation='relu'))\n",
    "                model.add(BatchNormalization())\n",
    "                model.add(Dropout(0.6))    # started with 0.5, increased to 0.7\n",
    "            \n",
    "            # output layer\n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "            \n",
    "            # Optional for resuming, load weights\n",
    "#             model.load_weights('weights/v2-10pct-5-conv-32-node-2-dens-1554945443.h5')\n",
    "\n",
    "            opt = Adam(lr=0.0008)   # Start with default of 0.001, slow down in subsequent epoch manually if needed\n",
    "            model.compile(loss='binary_crossentropy', \n",
    "                          optimizer=opt,\n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "#             model.fit_generator(generator=train_generator,\n",
    "#                                 steps_per_epoch=(train_generator.n // train_generator.batch_size),\n",
    "#                                 callbacks = callbacks,\n",
    "#                                 validation_data=val_generator,\n",
    "#                                 validation_steps=(val_generator.n // val_generator.batch_size),\n",
    "#                                 epochs=15,\n",
    "#                                 use_multiprocessing=False,\n",
    "#                                 workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('CNN-final-model-v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 194 of 228 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8167/8167 [==============================] - 388s 47ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "test_generator.reset()\n",
    "pred=model.predict_generator(test_generator,\n",
    "                            steps=test_generator.n//test_generator.batch_size,\n",
    "                            verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = df_test.gender.astype(int)\n",
    "y_pred = [1 if x>=0.5 else 0 for x in pred]\n",
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3658,  461],\n",
       "       [ 371, 3677]], dtype=int64)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "TN = cm[0][0]\n",
    "TP = cm[1][1]\n",
    "FN = cm[1][0]\n",
    "FP = cm[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8880796309783928"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN/(TN+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9083498023715415"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision (Male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.888593523441276"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP/(TP+FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision (Female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9079175974187144"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TN/(TN+FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8981266070772621"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(TP+TN)/(TN+TP+FN+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2611c074470>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sn.heatmap(cm, annot=True, square=True, cmap='Blues',  fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}